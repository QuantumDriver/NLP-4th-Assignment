{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Get data from web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "#å‚è€ƒæ–‡ç« ï¼šhttps://blog.csdn.net/HollyRan/article/details/85254375\n",
    "#ç›®æ ‡ç½‘ç«™ï¼šhttps://www.bjsubway.com/station/zjgls/#\n",
    "#ç™¾åº¦ç™¾ç§‘çš„ç«™ç‚¹ä¸å¤ªé è°±ï¼Œä»åŒ—äº¬åœ°é“å®˜ç½‘æ‰¾åˆ°çš„è¿™ä¸ªæ¯”è¾ƒå¥½ï¼Œè¿˜åŒ…æ‹¬ç«™ç‚¹è·ç¦»\n",
    "\n",
    "url = 'https://www.bjsubway.com/station/zjgls/#'\n",
    "#verifyå¿…é¡»ç­‰äºfalseï¼Œå¦åˆ™SSLä¼šæŠ¥é”™\n",
    "response = requests.get(url,verify=False)\n",
    "#ä½¿ç”¨ä¸­æ–‡ç¼–ç æ ¼å¼gbkè¿›è¡Œè§£ç \n",
    "response.encoding = 'gbk'\n",
    "html = response.text\n",
    "#å°†æºä»£ç ä¿¡æ¯ç”¨BeautifulSoupçš„parserè§£ç å™¨è¿›è¡Œè§£ç \n",
    "soup = BeautifulSoup(html,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<td colspan=\"5\">1å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨</td>, <td colspan=\"5\">5å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨</td>, <td colspan=\"5\">6å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨</td>, <td colspan=\"5\">7å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨</td>, <td colspan=\"5\">8å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨</td>, <td colspan=\"5\">9å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨</td>, <td colspan=\"5\">å…«é€šçº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨</td>, <td colspan=\"5\">äº¦åº„çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨</td>, <td colspan=\"5\">æˆ¿å±±çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨</td>, <td colspan=\"5\">æœºåœºçº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨</td>, <td colspan=\"6\">15å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨</td>, <td colspan=\"6\">æ˜Œå¹³çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨</td>, <td colspan=\"7\">2å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨</td>, <td colspan=\"7\">4å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨</td>, <td colspan=\"7\">13å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨</td>, <td colspan=\"7\">14å·çº¿(è¥¿æ®µ)ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨</td>, <td colspan=\"7\">14å·çº¿ï¼ˆä¸œæ®µï¼‰ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨</td>, <td colspan=\"7\">å¤§å…´çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨</td>, <td colspan=\"9\">10å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨</td>]\n",
      "\n",
      "\n",
      "['1å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨', '5å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨', '6å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨', '7å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨', '8å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨', '9å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨', 'å…«é€šçº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨', 'äº¦åº„çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨', 'æˆ¿å±±çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨', 'æœºåœºçº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨', '15å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨', 'æ˜Œå¹³çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨', '2å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨', '4å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨', '13å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨', '14å·çº¿(è¥¿æ®µ)ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨', '14å·çº¿ï¼ˆä¸œæ®µï¼‰ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨', 'å¤§å…´çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨', '10å·çº¿ç›¸é‚»ç«™é—´è·ä¿¡æ¯ç»Ÿè®¡è¡¨']\n"
     ]
    }
   ],
   "source": [
    "# å¾—åˆ°çº¿è·¯åç§°\n",
    "def get_txt_name():  \n",
    "    txt_src_name = []\n",
    "    for i in range(5, 10):\n",
    "        #è§‚å¯Ÿæºä»£ç å¯çŸ¥çº¿è·¯ä¿¡æ¯éƒ½åœ¨åä¸º\"colspan\"çš„å­—ç¬¦ä¸²åé¢ï¼Œä¸”colspanåé¢çš„æ•°å­—åœ¨5-10ä¹‹é—´\n",
    "        #ä½¿ç”¨soupè‡ªå¸¦çš„æ­£åˆ™è¡¨è¾¾ï¼Œå­—å…¸æ–¹å¼\n",
    "        temp = soup.find_all('td', {'colspan': str(i)})\n",
    "        txt_src_name += temp\n",
    "    return txt_src_name\n",
    "print(get_txt_name())\n",
    "#å¾—åˆ°å‡†ç¡®çº¿è·¯åç§°ï¼Œå»é™¤è¾¹è§’å†—ä½™å­—ç¬¦ä¸²\n",
    "def get_txtuseful_name(): \n",
    "    obj = []\n",
    "    for each in get_txt_name():\n",
    "        # ä»>åŒ¹é…åˆ°<(ä¸åŒ…å«)ï¼Œç”¨æ‹¬å·åŒ…ä½æƒ³è¦çš„éƒ¨åˆ†\n",
    "        temp = re.findall(r\">(.+?)<\", str(each))  \n",
    "        obj += temp\n",
    "    return obj\n",
    "print('\\n')\n",
    "#æŸ¥çœ‹åœ°é“çº¿è·¯æ€»å…±æœ‰å¤šå°‘\n",
    "print(get_txtuseful_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#åˆ†æç½‘é¡µæºç ï¼Œå‘ç°æ‰€æœ‰çš„ç«™ç‚¹ä¿¡æ¯éƒ½è¢«å­˜åœ¨\"<tbody>\"ä¸‹\n",
    "Station_info = soup.find_all('tbody')\n",
    "#print(Station_info)\n",
    "#tbodyä¸‹çš„å…³é”®ä¿¡æ¯ä»ç„¶éœ€è¦æ­£åˆ™æå–\n",
    "def get_station_info():\n",
    "    obj = []\n",
    "    for each in Station_info:\n",
    "        temp = re.findall(r\">(.+?)<\", str(each))   # æ­£åˆ™åŒ¹é…ï¼Œstræ ¼å¼\n",
    "        obj += temp\n",
    "    return obj\n",
    "#print(get_station_info())\n",
    "#å¾—åˆ°ç«™ç‚¹ä¸ä½ç½®ä¸¤ä¸ªå…³é”®ä¿¡æ¯\n",
    "station_geo_info = get_station_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Preprocessing data from page source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ä¿å­˜åˆ°testæ–‡ä»¶å¤‡ç”¨\n",
    "with open('test.txt','w',encoding='utf-8') as f:\n",
    "    for line in station_geo_info:\n",
    "        if line == 'ä¸Šè¡Œ/ä¸‹è¡Œ' or line == 'ä¸Šè¡Œ' or line == 'ä¸‹è¡Œ':\n",
    "            f.write('\\n')\n",
    "        else:\n",
    "            f.write(line + '   ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä¿å­˜åˆ°åˆ—è¡¨ä¸­\n",
    "info = []\n",
    "for i in station_geo_info:\n",
    "    #å»é™¤å¤šä½™çš„æ–‡å­—\n",
    "    if i == 'ä¸Šè¡Œ/ä¸‹è¡Œ' or i == 'ä¸Šè¡Œ' or i == 'ä¸‹è¡Œ': continue\n",
    "    else:\n",
    "        info.append(i)\n",
    "        \n",
    "info_1 = {}\n",
    "n = 0\n",
    "for i in info:\n",
    "    n += 1\n",
    "    #ç«™ç‚¹çš„å­—ç¬¦ä¸²å¤§äº5ï¼Œé€‰å‡ºç«™ç‚¹å½¢æˆå­—å…¸\n",
    "    #ç«™ç‚¹ä½œä¸ºkeyï¼Œè·ç¦»ä½œä¸ºå€¼ï¼ŒæŠŠç«™ç‚¹é—´çš„å­—ç¬¦ä¸²â€˜--â€™æ¢æˆâ€˜:â€™\n",
    "    if len(i)>5:\n",
    "        info_1[i.replace('â€”â€”',':')] = info[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å…ˆæ‰¾å‡ºç«™ç‚¹é—´å•å‘è¿æ¥çš„å…³ç³»\n",
    "station_connection_single = {}\n",
    "visited = []\n",
    "for i in info_1.keys():\n",
    "    #å¦‚æœæŸç«™ç‚¹åœ¨ä¹‹å‰å‡ºç°è¿‡ï¼Œé‚£ä¹ˆæ­¤ç«™ç‚¹è¿˜èƒ½åˆ°è¾¾å…¶ä»–ç‚¹\n",
    "    #å¦‚å»ºå›½é—¨å¯åˆ°åŒ—äº¬ç«™ï¼Œä¹Ÿå¯åˆ°ä¸œå•ï¼Œå…ˆè®¾{'å»ºå›½é—¨':['åŒ—äº¬ç«™']},å†æŠŠä¸œå•appendè¿›å…¥keyå¯¹åº”çš„valueä¸­\n",
    "    if i.split(':')[0] in visited:\n",
    "        station_connection_single[i.split(':')[0]].append(i.split(':')[1])\n",
    "    else:\n",
    "        station_connection_single[i.split(':')[0]]= [i.split(':')[1]]\n",
    "        visited.append(i.split(':')[0])\n",
    "#station_connection_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#è¡¥å……å‰åè¿æ¥å…³ç³»\n",
    "#ä¸€ä¸ªç«™ç‚¹é™¤äº†å¯ä»¥åˆ°å®ƒçš„ä¸‹ä¸€ä¸ªç‚¹ï¼Œå®ƒè¿˜å¯ä»¥å»åˆ°å®ƒçš„ä¸Šä¸€ä¸ªç‚¹\n",
    "station_connection = station_connection_single\n",
    "all_station = list(station_connection_single.keys())\n",
    "c = list(station_connection_single.values())\n",
    "for i in all_station:\n",
    "    for p in c:\n",
    "        #è‹¥æ­¤ç«™ç‚¹å‡ºç°åœ¨å…¶ä»–ç«™ç‚¹å¯ä»¥åˆ°è¾¾çš„ç«™ç‚¹ä¸­ï¼Œåˆ™æ­¤ç«™ç‚¹ä¹Ÿèƒ½åˆ°è¾¾é‚£ä¸ªç«™ç‚¹\n",
    "        #å¦‚å»ºå›½é—¨å¯åˆ°ç«™ç‚¹ä¸º[åŒ—äº¬ç«™ï¼Œä¸œå•],å¯¹äºåŒ—äº¬ç«™å’Œä¸œå•è€Œè¨€ï¼Œå®ƒä»¬ä¹Ÿå¯ä»¥åˆ°å»ºå›½é—¨\n",
    "        #æ‰¾åˆ°å»ºå›½é—¨çš„ç´¢å¼•å·ï¼Œèµ‹å…¥å­—å…¸keyå¯¹åº”çš„valueä¸­\n",
    "        if i in p:\n",
    "            index = c.index(p)\n",
    "            station_connection[i].append(all_station[index])        \n",
    "#station_connection     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['å¤©å®‰é—¨è¥¿', 'å®£æ­¦é—¨', 'å¤å…´é—¨', 'çµå¢ƒèƒ¡åŒ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_connection['è¥¿å•'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Build the search agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å’Œè¯¾å ‚ä»£ç ä¸€æ ·ï¼Œåˆ›å»ºæœç´¢ç­–ç•¥\n",
    "def search(start,stop,connection_graph,sort_candidate):\n",
    "    pathes = [[start]]\n",
    "    visited = set()\n",
    "    \n",
    "    while pathes:\n",
    "        path = pathes.pop()\n",
    "        frontier = path[-1]\n",
    "        if frontier in visited: continue\n",
    "        successors = connection_graph[frontier]\n",
    "        for city in successors:\n",
    "            if city in path: continue\n",
    "            new_path = path + [city]\n",
    "            pathes.append(new_path)\n",
    "            if city == stop: return new_path\n",
    "        visited.add(frontier)\n",
    "        pathes = sort_candidate(pathes)\n",
    "#æœ€é•¿ç«™ç‚¹\n",
    "def transfer_as_much(pathes):\n",
    "    return sorted(pathes,key=len)\n",
    "#æœ€å°‘ç«™ç‚¹\n",
    "def transfer_as_less(pathes):\n",
    "    return sorted(pathes,key=len,reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¯¼å…¥emoji\n",
    "from emoji import emojize\n",
    "def pretty_print(cities):\n",
    "    print(emojize(\" :metro:-> \").join(cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å»ºå›½é—¨ ğŸš‡-> ä¸œå• ğŸš‡-> ç‹åºœäº• ğŸš‡-> å¤©å®‰é—¨ä¸œ ğŸš‡-> å¤©å®‰é—¨è¥¿ ğŸš‡-> è¥¿å• ğŸš‡-> å¤å…´é—¨\n"
     ]
    }
   ],
   "source": [
    "pretty_print(search('å»ºå›½é—¨','å¤å…´é—¨',station_connection,sort_candidate=transfer_as_less))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å»ºå›½é—¨ ğŸš‡-> åŒ—äº¬ç«™ ğŸš‡-> å´‡æ–‡é—¨ ğŸš‡-> å‰é—¨ ğŸš‡-> å’Œå¹³é—¨ ğŸš‡-> å®£æ­¦é—¨ ğŸš‡-> é•¿æ¤¿è¡— ğŸš‡-> å¤å…´é—¨\n"
     ]
    }
   ],
   "source": [
    "pretty_print(search('å»ºå›½é—¨','å¤å…´é—¨',station_connection,sort_candidate=transfer_as_much))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¸œå• ğŸš‡-> ç¯å¸‚å£ ğŸš‡-> ä¸œå›› ğŸš‡-> å—é”£é¼“å·· ğŸš‡-> ä»€åˆ¹æµ· ğŸš‡-> é¼“æ¥¼å¤§è¡— ğŸš‡-> å®‰å¾·é‡ŒåŒ—è¡— ğŸš‡-> å®‰åæ¡¥ ğŸš‡-> åŒ—åœŸåŸ ğŸš‡-> å¥¥ä½“ä¸­å¿ƒ ğŸš‡-> å¥¥æ—åŒ¹å…‹å…¬å›­ ğŸš‡-> åŒ—æ²™æ»© ğŸš‡-> å…­é“å£\n"
     ]
    }
   ],
   "source": [
    "pretty_print(search('ä¸œå•','å…­é“å£',station_connection,sort_candidate=transfer_as_less))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
